{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GAN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIgEXYGN_2fq"
      },
      "source": [
        "#!unzip /content/blood.zip  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZ9UJ9XwspYL"
      },
      "source": [
        "from keras import backend as K\n",
        "import keras\n",
        "from keras.layers import Input, Dense, Reshape, Flatten, Concatenate\n",
        "from keras.layers import BatchNormalization, Activation, Embedding, multiply\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.layers.convolutional import Conv2D, Conv2DTranspose\n",
        "from keras.optimizers import Adam\n",
        "from keras.models import Model, Sequential\n",
        "from keras.utils import to_categorical\n",
        "from keras.datasets.mnist  import load_data\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SbdSo3JOsrF5"
      },
      "source": [
        "img_shape = (28, 28, 1)\n",
        "z_dim = 100\n",
        "num_classes = 10\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAiXPiN2FhY2"
      },
      "source": [
        "# Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUkP3EivFOol"
      },
      "source": [
        "def build_generator(z_dim):\n",
        "    \n",
        "    model = Sequential()\n",
        "    \n",
        "    model.add(Dense(7*7*256, input_shape=(z_dim, )))\n",
        "    model.add(Reshape((7, 7, 256)))\n",
        "    \n",
        "    # 7*7*256 => 14*14*128\n",
        "    model.add(Conv2DTranspose(128, kernel_size=3, strides=2, padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU(alpha=0.01))\n",
        "    \n",
        "    # 14*14*128 => 14*14*64\n",
        "    model.add(Conv2DTranspose(64, kernel_size=3, strides=1, padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU(alpha=0.01))\n",
        "    \n",
        "    # 14*14*64 => 28*28*1\n",
        "    model.add(Conv2DTranspose(1, kernel_size=3, strides=2, padding='same'))\n",
        "    model.add(Activation('tanh'))\n",
        "    \n",
        "    z = Input(shape=(z_dim, ))\n",
        "    \n",
        "    # Conditioning label\n",
        "    label = Input(shape=(1,), dtype='int32')\n",
        "    \n",
        "    # embedding layer:\n",
        "    # turns labels into dense vectors of size z_dim\n",
        "    # produces 3D tensor with shape: (batch_size, 1, z_dim)\n",
        "    label_embedding = Embedding(num_classes, z_dim, input_length=1)(label)\n",
        "    \n",
        "    # Flatten the embedding 3D tensor into 2D  tensor with shape: (batch_size, z_dim)\n",
        "    label_embedding = Flatten()(label_embedding)\n",
        "    \n",
        "    # Element-wise product of the vectors z and the label embeddings\n",
        "    joined_representation = multiply([z, label_embedding])\n",
        "    \n",
        "    img = model(joined_representation)\n",
        "    \n",
        "    return Model([z, label], img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVV4yWFBFdHk"
      },
      "source": [
        "# Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbSo8057tpnw"
      },
      "source": [
        "def build_discriminator(img_shape):\n",
        "    \n",
        "    model = Sequential()\n",
        "    \n",
        "    # 28*28*2 => 14*14*64\n",
        "    model.add(Conv2D(64, kernel_size=3, strides=2, padding='same', input_shape=(28, 28, 2)))\n",
        "    model.add(LeakyReLU(alpha=0.01))\n",
        "    \n",
        "    # 14*14*64 => 7*7*64\n",
        "    model.add(Conv2D(64, kernel_size=3, strides=2, padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU(alpha=0.01))\n",
        "    \n",
        "    # 7*7*128 => 3*3*128\n",
        "    model.add(Conv2D(128, kernel_size=3, strides=2, padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU(alpha=0.01))\n",
        "    \n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    \n",
        "    img = Input(shape=img_shape)\n",
        "    \n",
        "    label = Input(shape=(1,), dtype='int32')\n",
        "    \n",
        "    # embedding layer:\n",
        "    # turns labels into dense vectors of size 28*28*1\n",
        "    # produces 3D tensor with shape: (batch_size, 1, 28*28*1)\n",
        "    label_embedding = Embedding(input_dim=num_classes, output_dim=np.prod(img_shape), input_length=1)(label)\n",
        "    # Flatten the embedding 3D tensor into 2D  tensor with shape: (batch_size, 28*28*1)\n",
        "    label_embedding = Flatten()(label_embedding)\n",
        "    # Reshape label embeddings to have same dimensions as input images\n",
        "    label_embedding = Reshape(img_shape)(label_embedding)\n",
        "    \n",
        "    # concatenate images with corresponding label embeddings\n",
        "    concatenated = Concatenate(axis=-1)([img, label_embedding])\n",
        "    \n",
        "    prediction = model(concatenated)\n",
        "    \n",
        "    return Model([img, label], prediction)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KItzkxdUFYfu"
      },
      "source": [
        "# Model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ziq4lyT5FXqM"
      },
      "source": [
        "# building and compiling the Discriminator\n",
        "disc = build_discriminator(img_shape)\n",
        "disc.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer=Adam())\n",
        "\n",
        "# build the generator\n",
        "gen = build_generator(z_dim)\n",
        "\n",
        "# the generator takes noise and the target label as input\n",
        "# and generates the corresponding digit for that label\n",
        "z = Input(shape=(z_dim,))\n",
        "label = Input(shape=(1,))\n",
        "\n",
        "img = gen([z, label])\n",
        "\n",
        "# keep the discriminator's params constant for generator training\n",
        "disc.trainable = False\n",
        "\n",
        "prediction = disc([img, label])\n",
        "\n",
        "# Conditional (Conditional) GAN model with fixed discriminator to train the generator\n",
        "cgan = Model([z, label], prediction)\n",
        "cgan.compile(loss='binary_crossentropy', optimizer=Adam())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMYMF3mJFn1P"
      },
      "source": [
        "# Examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xseQ-8d0FqbF"
      },
      "source": [
        "def sample_images(image_grid_rows=2, image_grid_columns=5):\n",
        "    z = np.random.normal(0, 1, (image_grid_rows * image_grid_columns, z_dim))\n",
        "    labels = np.arange(0, 10).reshape(-1, 1)\n",
        "    gen_imgs = gen.predict([z, labels])\n",
        "    gen_imgs = 0.5 * gen_imgs + 0.5\n",
        "    fig, axs = plt.subplots(image_grid_rows, image_grid_columns, figsize=(10,4), sharey=True, sharex=True)\n",
        "    cnt = 0\n",
        "    for i in range(image_grid_rows):\n",
        "        for j in range(image_grid_columns):\n",
        "            axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
        "            axs[i,j].axis('off')\n",
        "            axs[i,j].set_title(\"Digit: %d\" % labels[cnt])\n",
        "            cnt += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAZhYufHFrP0"
      },
      "source": [
        "# Training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25EW07HVGKPn"
      },
      "source": [
        "## load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_Pyz9AsFuFh"
      },
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3FvXOVtGMUV"
      },
      "source": [
        "## Train function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pAuMzMrKGG4w"
      },
      "source": [
        "accuracies = []\n",
        "losses = []\n",
        "\n",
        "def train(iterations, batch_size, sample_interval):\n",
        "    \n",
        "    (X_train, y_train), (_, _) = load_data()\n",
        "    \n",
        "   # X_train = (X_train - 127.5) / 127.5\n",
        "    X_train = np.expand_dims(X_train, axis=3)\n",
        "\n",
        "    X_train = X_train.reshape(-1, 28, 28, 1).astype('float32')\n",
        "    X_train = X_train /256 \n",
        "\n",
        "    #X_test = X_test.reshape(-1, 28, 28, 1).astype('float32')\n",
        "    #X_test = X_test /256 \n",
        "\n",
        "    real = np.ones(shape=(batch_size, 1))\n",
        "    fake = np.zeros(shape=(batch_size, 1))\n",
        "    \n",
        "    for iteration in range(iterations):\n",
        "        \n",
        "        idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
        "        imgs, labels = X_train[idx], y_train[idx]\n",
        "        \n",
        "        z = np.random.normal(0, 1, size=(batch_size, z_dim))\n",
        "        gen_imgs = gen.predict([z, labels])\n",
        "        \n",
        "        d_loss_real = disc.train_on_batch([imgs, labels], real)\n",
        "        d_loss_fake = disc.train_on_batch([gen_imgs, labels], fake)\n",
        "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "        \n",
        "        z = np.random.normal(0, 1, size=(batch_size, z_dim))\n",
        "        labels = np.random.randint(0, num_classes, batch_size).reshape(-1, 1)\n",
        "        \n",
        "        g_loss = cgan.train_on_batch([z, labels], real)\n",
        "        \n",
        "        if iteration % sample_interval == 0:\n",
        "            print('{} [D loss: {}, accuracy: {:.2f}] [G loss: {}]'.format(iteration, d_loss[0], 100 * d_loss[1], g_loss))\n",
        "        \n",
        "            losses.append((d_loss[0], g_loss))\n",
        "            accuracies.append(d_loss[1])\n",
        "            \n",
        "            sample_images()\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmNqTxEmGR34"
      },
      "source": [
        "Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4_ikIQmGRRA"
      },
      "source": [
        "iterations = 20000\n",
        "batch_size = 128\n",
        "sample_interval = 1000\n",
        "\n",
        "train(iterations, batch_size, sample_interval)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-Ks_uYkNAju"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_sp8nYuFrFxX"
      },
      "source": [
        " gen.predict([np.random.normal(0, 1, (5 * 2, z_dim)), np.arange(0, 10).reshape(-1, 1)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIFkKaqwaPJi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "b4cedcf4-4b99-4660-f17c-318c84fb3e67"
      },
      "source": [
        "\n",
        "z = np.random.normal(0, 1, (5 * 2, z_dim))\n",
        "labels = np.arange(0, 10).reshape(-1, 1)\n",
        "gen_imgs = gen.predict([z, labels])\n",
        "plt.imshow(gen_imgs[0, :,:,0], cmap='gray')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fe95da11c50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 137
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Denoisy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jH2-y9NhQhRL"
      },
      "source": [
        "import keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5P9g1TdhQhi1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "outputId": "a5fc55d8-680e-4ffc-9732-be2e105bb3cb"
      },
      "source": [
        "gan = keras.models.Model()\n",
        "denoisy = keras.models.load_model('/content/autoencoder_denoisy.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-cbb4f4538dff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/gan.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdenoisy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/autoencoder_denoisy.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/saving/save.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[1;32m    180\u001b[0m     if (h5py is not None and (\n\u001b[1;32m    181\u001b[0m         isinstance(filepath, h5py.File) or h5py.is_hdf5(filepath))):\n\u001b[0;32m--> 182\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mhdf5_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model_from_hdf5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0mfilepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath_to_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/saving/hdf5_format.py\u001b[0m in \u001b[0;36mload_model_from_hdf5\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0mmodel_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model_config'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmodel_config\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'No model found in config file.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m     \u001b[0mmodel_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m     model = model_config_lib.model_from_config(model_config,\n",
            "\u001b[0;31mValueError\u001b[0m: No model found in config file."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WgqYk65aQhKi"
      },
      "source": [
        "\n",
        "# Sample random points in the latent space\n",
        "random_latent_vectors = np.random.normal(size=(10, latent_dim))\n",
        "\n",
        "# Decode them to fake images\n",
        "generated_images = generator.predict(random_latent_vectors)\n",
        "\n",
        "for i in range(generated_images.shape[0]):\n",
        "    img = image.array_to_img(generated_images[i] * 255., scale=False)\n",
        "    plt.figure()\n",
        "    plt.imshow(img ,cmap='gray')\n",
        "    \n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}